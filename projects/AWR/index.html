---
layout: default
title: "Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning"
---

<center><h1>{{ page.title }}</h1></center>

<td>
	<center>
		arXiv Preprint 2019<br>
		<br>
		<nobr>Xue Bin Peng</nobr> &emsp;&emsp; <nobr>Aviral Kumar</nobr> &emsp;&emsp; <nobr>Grace Zhang</nobr> &emsp;&emsp; <nobr>Sergey Levine</nobr><br>
		<br>
		<nobr>University of California, Berkeley</nobr><br>
		<br>
		<img style="vertical-align:middle" src="awr_teaser.png"  width="100%" height="inherit"/>		
	</center>
</td>

<br>
	
<td>
	<hr>
	<h3 style="margin-bottom:10px;">Abstract</h3>
	In this paper, we aim to develop a simple and scalable reinforcement learning algorithm
	that uses standard supervised learning methods as subroutines. Our goal is an algorithm
	that utilizes only simple and convergent maximum likelihood loss functions, while also
	being able to leverage off-policy data. Our proposed approach, which we refer to as 
	advantage-weighted regression (AWR), consists of two standard supervised learning steps:
	one to regress onto target values for a value function, and another to regress onto 
	weighted target actions for the policy. The method is simple and general, can accommodate
	continuous and discrete actions, and can be implemented in just a few lines of code on top
	of standard supervised learning methods. We provide a theoretical motivation for AWR and 
	analyze its properties when incorporating off-policy data from experience replay. We 
	evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves
	competitive performance compared to a number of well-established state-of-the-art RL 
	algorithms. AWR is also able to acquire more effective policies than most off-policy
	algorithms when learning from purely static datasets with no additional environmental
	interactions. Furthermore, we demonstrate our algorithm on challenging continuous control
	tasks with highly complex simulated characters.
</td>

<td>
	<h3> Paper: [<a href="AWR_2019.pdf">PDF</a>] &nbsp; &nbsp; &nbsp; Code: [<a href="https://github.com/xbpeng/MimicKit">MimicKit</a> / <a href="https://github.com/xbpeng/awr">GitHub</a>] &nbsp; &nbsp; &nbsp; Preprint: [<a href="https://arxiv.org/abs/1910.00177">arXiv</a>] </h3>
</td>

<tr>
		<h3 style="margin-bottom:10px;">Video</h3>
		<iframe width="560" height="315" src="https://www.youtube.com/embed/ROwJ_O2NINc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</tr>
	
<br>
<br>

<h3 style="margin-bottom:0px;">Bibtex</h3>
<pre>
@article{
	AWRPeng19,
	author = {Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},
	title = {Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},
	journal = {CoRR},
	volume = {abs/1910.00177},
	year = {2019},
	url = {https://arxiv.org/abs/1910.00177},
	archivePrefix = {arXiv},
	eprint = {1910.00177},
	timestamp = {Tue, 01 October 2019 11:27:50 +0200},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
</pre>
