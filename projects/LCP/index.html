---
layout: default
title: "Learning Smooth Humanoid Locomotion through Lipschitz-Constrained Policies"
---

<center><h1>{{ page.title }}</h1></center>

<td>
	<center>
		arXiv Preprint 2024<br>
		<br>
		<nobr>Zixuan Chen* (1)</nobr> &emsp;&emsp; <nobr>Xialin He* (2)</nobr> &emsp;&emsp; <nobr>Yen-Jen Wang* (3)</nobr> &emsp;&emsp; <nobr>Qiayuan Liao (3)</nobr> &emsp;&emsp; <nobr>Yanjie Ze (4)</nobr> &emsp;&emsp; <nobr>Zhongyu Li (3)</nobr> &emsp;&emsp; <nobr>S. Shankar Sastry (3)</nobr> &emsp;&emsp; <nobr>Jiajun Wu (4)</nobr> &emsp;&emsp; <nobr>Koushil Sreenath (3)</nobr> &emsp;&emsp; <nobr>Saurabh Gupta (2)</nobr> &emsp;&emsp; <nobr>Xue Bin Peng (1, 5)</nobr> <br>
		<br>
		<nobr>(1) Simon Fraser University</nobr> &emsp;&emsp; <nobr>(2) UIUC 3UC Berkeley</nobr> &emsp;&emsp; <nobr>(4) Stanford University</nobr> &emsp;&emsp; <nobr>(5) NVIDIA</nobr><br>
		<br>
		<nobr>*Equal contribution.</nobr><br>
		<br>
		<img style="vertical-align:middle" src="lcp_teaser.png"  width="100%" height="inherit"/>		
	</center>
</td>

<br>
	
<td>
	<hr>
	<h3 style="margin-bottom:10px;">Abstract</h3>
	Reinforcement learning combined with sim-to-real transfer offers a general framework
	for developing locomotion controllers for legged robots. To facilitate successful
	deployment in the real world, smoothing techniques, such as low-pass filters and
	smoothness rewards, are often employed to develop policies with smooth behaviors.
	However, because these techniques are non-differentiable and usually require tedious
	tuning of a large set of hyperparameters, they tend to require extensive manual
	tuning for each robotic platform. To address this challenge and establish a general
	technique for enforcing smooth behaviors, we propose a simple and effective method
	that imposes a Lipschitz constraint on a learned policy, which we refer to as
	Lipschitz-Constrained Policies (LCP). We show that the Lipschitz constraint can be
	implemented in the form of a gradient penalty, which provides a differentiable
	objective that can be easily incorporated with automatic differentiation frameworks.
	We demonstrate that LCP effectively replaces the need for smoothing rewards or
	low-pass filters and can be easily integrated into training frameworks for many
	distinct humanoid robots. We extensively evaluate LCP in both simulation and
	real-world humanoid robots, producing smooth and robust locomotion controllers.
</td>

<td>
	<h3> Paper: [<a href="LCP_2024.pdf">PDF</a>] &nbsp; &nbsp; &nbsp; Code: [<a href="https://github.com/zixuan417/smooth-humanoid-locomotion">GitHub</a>] &nbsp; &nbsp; &nbsp; Webpage: [<a href="https://lipschitz-constrained-policy.github.io/">Link</a>] &nbsp; &nbsp; &nbsp; Preprint: [<a href="https://arxiv.org/abs/2410.11825">arXiv</a>] </h3>
</td>

<tr>
	<h3 style="margin-bottom:10px;">Video</h3>
	<iframe width="560" height="315" src="https://www.youtube.com/embed/4mJCFZ--0sc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</tr>
	
<br>
<br>

<h3 style="margin-bottom:0px;">Bibtex</h3>
<pre>
@article{
	chen2024lcp,
	title = {Learning Smooth Humanoid Locomotion through Lipschitz-Constrained Policies},
	author = {Zixuan Chen and Xialin He and Yen-Jen Wang and Qiayuan Liao and Yanjie Ze and Zhongyu Li and S. Shankar Sastry and Jiajun Wu and Koushil Sreenath and Saurabh Gupta and Xue Bin Peng},
	journal = {arxiv preprint arXiv:2410.11825},
	year = {2024}
}
</pre>
