---
layout: default
title: "Synthesizing Physical Character-Scene Interactions"
---

<center><h1>{{ page.title }}</h1></center>

<td>
	<center>
		ACM SIGGRAPH 2023<br>
		<br>
		<nobr> Mohamed Hassan (1)</nobr> &emsp;&emsp; <nobr>Yunrong Guo (2)</nobr> &emsp;&emsp; <nobr>Tingwu Wang (2)</nobr> &emsp;&emsp; <nobr>Michael Black (3)</nobr> &emsp;&emsp; <nobr>Sanja Fidler (4, 2)</nobr> &emsp;&emsp; <nobr>Xue Bin Peng (5, 2)</nobr> <br>
		<br>
		<nobr>(1) Electronic Arts</nobr> &emsp;&emsp; <nobr>(2) NVIDIA </nobr> &emsp;&emsp; <nobr>(3) Max-Planck-Institute for Intelligent Systems</nobr> &emsp;&emsp; <nobr>(4) University of Toronto </nobr> &emsp;&emsp; <nobr>(5) Simon Fraser University </nobr><br>
		<br>
		<img style="vertical-align:middle" src="inter_phys_teaser.png"  width="100%" height="inherit"/>		
	</center>
</td>

<br>
	
<td>
	<hr>
	<h3 style="margin-bottom:10px;">Abstract</h3>
	Movement is how people interact with and affect their environment. For realistic
	character animation, it is necessary to synthesize such interactions between
	virtual characters and their surroundings. Despite recent progress in character
	animation using machine learning, most systems focus on controlling an agent's
	movements in fairly simple and homogeneous environments, with limited
	interactions with other objects. Furthermore, many previous approaches that
	synthesize human-scene interactions require significant manual labeling of the
	training data. In contrast, we present a system that uses adversarial imitation
	learning and reinforcement learning to train physically-simulated characters
	that perform scene interaction tasks in a natural and life-like manner. Our
	method learns scene interaction behaviors from large unstructured motion
	datasets, without manual annotation of the motion data. These scene
	interactions are learned using an adversarial discriminator that evaluates
	the realism of a motion within the context of a scene. The key novelty involves
	conditioning both the discriminator and the policy networks on scene context.
	We demonstrate the effectiveness of our approach through three challenging
	scene interaction tasks: carrying, sitting, and lying down, which require
	coordination of a character's movements in relation to objects in the
	environment. Our policies learn to seamlessly transition between different
	behaviors like idling, walking, and sitting. By randomizing the properties
	of the objects and their placements during training, our method is able to
	generalize beyond the objects and scenarios depicted in the training dataset,
	producing natural character-scene interactions for a wide variety of object
	shapes and placements.
</td>

<td>
	<h3> Paper: [<a href="InterPhys_2023.pdf">PDF</a>] &nbsp; &nbsp; &nbsp; Preprint: [<a href="https://arxiv.org/abs/2302.00883">arXiv</a>] </h3>
</td>

<tr>
		<h3 style="margin-bottom:10px;">Videos</h3>
		<iframe width="560" height="315" src="https://www.youtube.com/embed/q3hyQdaElQQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</tr>
	
<br>
<br>

<h3 style="margin-bottom:0px;">Bibtex</h3>
<pre>
@article{
	InterPhysHassan2023,
	author = {Hassan, Mohamed and Guo, Yunrong and Wang, Tingwu and Black, Michael and Fidler, Sanja and Peng, Xue Bin},
	title = {Synthesizing Physical Character-Scene Interactions},
	year = {2023},
	isbn = {9798400701597},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3588432.3591525},
	doi = {10.1145/3588432.3591525},
	booktitle = {ACM SIGGRAPH 2023 Conference Proceedings},
	articleno = {63},
	numpages = {9},
	keywords = {character animation, reinforcement learning, unsupervised reinforcement learning, adversarial imitation learning},
	location = {Los Angeles, CA, USA},
	series = {SIGGRAPH '23}
}
</pre>
