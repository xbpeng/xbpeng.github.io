---
layout: default
title: "Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives"
---

<center><h1>{{ page.title }}</h1></center>

<td>
	<center>
		International Conference on Learning Representations (ICLR 2020)<br>
		<br>
		<nobr>Anirudh Goyal (1)</nobr> &emsp;&emsp; <nobr>Shagun Sodhani (1)</nobr> &emsp;&emsp; <nobr>Jonathan Binas (1)</nobr> &emsp;&emsp; <nobr>Xue Bin Peng (2)</nobr> &emsp;&emsp; <nobr>Sergey Levine (2)</nobr> &emsp;&emsp; <nobr>Yoshua Bengio (1)</nobr>
		<br>
		<nobr>(1) Mila, Université de Montréal</nobr> &emsp;&emsp; <nobr>(2) University of California, Berkeley</nobr><br>
		<br>
		<img style="vertical-align:middle" src="cicp_teaser.png"  width="60%" height="inherit"/>		
	</center>
</td>

<br>
	
<td>
	<hr>
	<h3 style="margin-bottom:10px;">Abstract</h3>
	Reinforcement learning agents that operate in diverse and complex environments
	can benefit from the structured decomposition of their behavior. Often, this is
	addressed in the context of hierarchical reinforcement learning, where the aim is
	to decompose a policy into lower-level primitives or options, and a higher-level
	meta-policy that triggers the appropriate behaviors for a given situation. However,
	the meta-policy must still produce appropriate decisions in all states. In this
	work, we propose a policy design that decomposes into primitives, similarly to
	hierarchical reinforcement learning, but without a high-level meta-policy. Instead,
	each primitive can decide for themselves whether they wish to act in the current
	state. We use an information-theoretic mechanism for enabling this decentralized
	decision: each primitive chooses how much information it needs about the current
	state to make a decision and the primitive that requests the most information about
	the current state acts in the world. The primitives are regularized to use as little
	information as possible, which leads to natural competition and specialization. We
	experimentally demonstrate that this policy architecture improves over both flat
	and hierarchical policies in terms of generalization.
</td>

<td>
	<h3> Paper: [<a href="CICP_2020.pdf">PDF</a>] &nbsp; &nbsp; &nbsp; Preprint: [<a href="https://arxiv.org/abs/1906.10667">arXiv</a>] </h3>
</td>

<br>

<h3 style="margin-bottom:0px;">Bibtex</h3>
<pre>
@inproceedings{
	goyal2020reinforcement,
	title={Reinforcement Learning with Competitive  Ensembles of Information-Constrained Primitives},
	author={Anirudh Goyal and Shagun Sodhani and Jonathan Binas and Xue Bin Peng and Sergey Levine and Yoshua Bengio},
	booktitle={International Conference on Learning Representations},
	year={2020},
	url={https://openreview.net/forum?id=ryxgJTEYDr}
}
</pre>
