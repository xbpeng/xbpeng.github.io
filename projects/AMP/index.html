---
layout: default
title: "AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control"
---

<center><h1>{{ page.title }}</h1></center>

<td>
	<center>
		Transactions on Graphics (Proc. ACM SIGGRAPH 2021)<br>
		<br>
		<nobr>Xue Bin Peng* (1)</nobr> &emsp;&emsp; <nobr>Ze Ma* (2)</nobr> &emsp;&emsp; <nobr>Pieter Abbeel (1)</nobr> &emsp;&emsp; <nobr>Sergey Levine (1)</nobr> &emsp;&emsp; <nobr>Angjoo Kanazawa (1)</nobr> <br>
		<br>
		<nobr>(1) University of California, Berkeley</nobr> &emsp;&emsp; <nobr>(2) Shanghai Jiao Tong University </nobr><br>
		<br>
		<nobr>*Joint first authors.</nobr><br>
		<br>
		<img style="vertical-align:middle" src="amp_teaser.png"  width="100%" height="inherit"/>		
	</center>
</td>

<br>
	
<td>
	<hr>
	<h3 style="margin-bottom:10px;">Abstract</h3>
	Synthesizing graceful and life-like behaviors for physically simulated characters
	has been a fundamental challenge in computer animation. Data-driven
	methods that leverage motion tracking are a prominent class of techniques
	for producing high fidelity motions for a wide range of behaviors. However,
	the effectiveness of these tracking-based methods often hinges on carefully
	designed objective functions, and when applied to large and diverse motion
	datasets, these methods require significant additional machinery to select the
	appropriate motion for the character to track in a given scenario. In thiswork,
	we propose to obviate the need to manually design imitation objectives and
	mechanisms for motion selection by utilizing a fully automated approach
	based on adversarial imitation learning. High-level task objectives that the
	character should perform can be specified by relatively simple reward functions,
	while the low-level style of the characterâ€™s behaviors can be specified
	by a dataset of unstructured motion clips, without any explicit clip selection
	or sequencing. For example, a character traversing an obstacle course might
	utilize a task-reward that only considers forward progress, while the dataset
	contains clips of relevant behaviors such as running, jumping, and rolling.
	These motion clips are used to train an adversarial motion prior, which specifies
	style-rewards for training the character through reinforcement learning
	(RL). The adversarial RL procedure automatically selects which motion to
	perform, dynamically interpolating and generalizing from the dataset. Our
	system produces high-quality motions that are comparable to those achieved
	by state-of-the-art tracking-based techniques, while also being able to easily
	accommodate large datasets of unstructured motion clips. Composition of
	disparate skills emerges automatically from the motion prior, without requiring
	a high-level motion planner or other task-specific annotations of
	the motion clips. We demonstrate the effectiveness of our framework on
	a diverse cast of complex simulated characters and a challenging suite of
	motor control tasks.
</td>

<td>
	<h3> Paper: [<a href="2021_TOG_AMP.pdf">PDF</a>] &nbsp; &nbsp; &nbsp; Code: [<a href="https://github.com/xbpeng/DeepMimic">GitHub</a>] </h3>
</td>

<tr>
		<h3 style="margin-bottom:10px;">Videos</h3>
		<iframe width="560" height="315" src="https://www.youtube.com/embed/wySUxZN_KbM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
		<br><br>
		<iframe width="560" height="315" src="https://www.youtube.com/embed/O6fBSMxThR4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</tr>
	
<br>
<br>

<h3 style="margin-bottom:0px;">Bibtex</h3>
<pre>
@article{
	2021-TOG-AMP,
	author = {Peng, Xue Bin and Ma, Ze and Abbeel, Pieter and Levine, Sergey and Kanazawa, Angjoo},
	title = {AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control},
	journal = {ACM Trans. Graph.},
	issue_date = {August 2021},
	volume = {40},
	number = {4},
	month = jul,
	year = {2021},
	articleno = {1},
	numpages = {15},
	url = {http://doi.acm.org/10.1145/3450626.3459670},
	doi = {10.1145/3450626.3459670},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {motion control, physics-based character animation, reinforcement learning},
} 
</pre>
