---
layout: default
title: "DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills"
---

<center><h1>{{ page.title }}</h1></center>

<td>
	<center>
		Transactions on Graphics (Proc. ACM SIGGRAPH 2018)<br>
		<br>
		<nobr>Xue Bin Peng (1)</nobr> &emsp;&emsp; <nobr>Pieter Abbeel (1)</nobr> &emsp;&emsp; <nobr>Sergey Levine (1)</nobr> &emsp;&emsp; <nobr>Michiel van de Panne (2)</nobr><br>
		<br>
		<nobr>(1) University of California, Berkeley</nobr> &emsp;&emsp; <nobr>(2) University of British Columbia</nobr><br>
		<br>
		<img style="vertical-align:middle" src="deepmimic_teaser.png"  width="100%" height="inherit"/>		
	</center>
</td>

<br>
	
<td>
	<hr>
	<h3 style="margin-bottom:10px;">Abstract</h3>
	A longstanding goal in character animation is to combine data-driven specification
	of behavior with a system that can execute a similar behavior in a
	physical simulation, thus enabling realistic responses to perturbations and
	environmental variation. We show that well-known reinforcement learning
	(RL) methods can be adapted to learn robust control policies capable of imitating
	a broad range of example motion clips, while also learning complex
	recoveries, adapting to changes in morphology, and accomplishing userspecified
	goals. Our method handles keyframed motions, highly-dynamic
	actions such as motion-captured flips and spins, and retargeted motions. By
	combining a motion-imitation objective with a task objective, we can train
	characters that react intelligently in interactive settings, e.g., by walking in a
	desired direction or throwing a ball at a user-specified target. This approach
	thus combines the convenience and motion quality of using motion clips to
	define the desired style and appearance, with the flexibility and generality
	afforded by RL methods and physics-based animation. We further explore a
	number of methods for integrating multiple clips into the learning process
	to develop multi-skilled agents capable of performing a rich repertoire of
	diverse skills. We demonstrate results using multiple characters (human,
	Atlas robot, bipedal dinosaur, dragon) and a large variety of skills, including
	locomotion, acrobatics, and martial arts.
</td>

<td>
	<h3> Paper: [<a href="DeepMimic_2018.pdf">PDF</a>] &nbsp; &nbsp; &nbsp; Code: [<a href="https://github.com/xbpeng/DeepMimic">GitHub</a>] &nbsp; &nbsp; &nbsp; Media: [<a href="http://bair.berkeley.edu/blog/2018/04/10/virtual-stuntman/">BAIR</a> / <a href="https://www.technologyreview.com/2018/04/10/144007/virtual-robots-that-teach-themselves-kung-fu-could-revolutionize-video-games/">Tech Review</a>] &nbsp; &nbsp; &nbsp; Preprint: [<a href="https://arxiv.org/abs/1804.02717">arXiv</a>] </h3>
</td>

<tr>
		<h3 style="margin-bottom:10px;">Videos</h3>
		<iframe width="560" height="315" src="https://www.youtube.com/embed/vppFvq2quQ0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
		<br><br>
		<iframe width="560" height="315" src="https://www.youtube.com/embed/8KdDwRLtNHQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</tr>
	
<br>
<br>

<h3 style="margin-bottom:0px;">Bibtex</h3>
<pre>
@article{
	2018-TOG-deepMimic,
	author = {Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and van de Panne, Michiel},
	title = {DeepMimic: Example-guided Deep Reinforcement Learning of Physics-based Character Skills},
	journal = {ACM Trans. Graph.},
	issue_date = {August 2018},
	volume = {37},
	number = {4},
	month = jul,
	year = {2018},
	issn = {0730-0301},
	pages = {143:1--143:14},
	articleno = {143},
	numpages = {14},
	url = {http://doi.acm.org/10.1145/3197517.3201311},
	doi = {10.1145/3197517.3201311},
	acmid = {3201311},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {motion control, physics-based character animation, reinforcement learning},
} 
</pre>
