---
layout: default
title: "Legged Robots that Keep on Learning: Fine-Tuning Locomotion Policies in the Real World"
---

<center><h1>{{ page.title }}</h1></center>

<td>
	<center>
		arXiv Preprint 2021<br>
		<br>
		<nobr>Laura Smith (1)</nobr> &emsp;&emsp; <nobr> J. Chase Kew (2)</nobr> &emsp;&emsp; <nobr> Xue Bin Peng (1)</nobr> &emsp;&emsp; <nobr> Sehoon Ha (2,3)</nobr> &emsp;&emsp; <nobr>  Jie Tan (2) </nobr> &emsp;&emsp; <nobr> Sergey Levine (1,2)</nobr> <br>
		<br>
		<nobr>(1) University of California, Berkeley</nobr> &emsp;&emsp; <nobr>(2) Google Research </nobr> &emsp;&emsp; <nobr>(3) Georgia Institute of Technology </nobr><br>
		<br>
		<img style="vertical-align:middle" src="finetuning_locomotion_teaser.png"  width="100%" height="inherit"/>		
	</center>
</td>

<br>
	
<td>
	<hr>
	<h3 style="margin-bottom:10px;">Abstract</h3>
	Legged robots are physically capable of traversing a wide range of challenging
	environments, but designing controllers that are sufficiently robust to handle
	this diversity has been a long-standing challenge in robotics. Reinforcement
	learning presents an appealing approach for automating the controller design
	process and has been able to produce remarkably robust controllers when trained
	in a suitable range of environments. However, it is difficult to predict all
	likely conditions the robot will encounter during deployment and enumerate them
	at training-time. What if instead of training controllers that are robust enough
	to handle any eventuality, we enable the robot to continually learn in any
	setting it finds itself in? This kind of real-world reinforcement learning poses
	a number of challenges, including efficiency, safety, and autonomy. To address
	these challenges, we propose a practical robot reinforcement learning system for
	fine-tuning locomotion policies in the real world. We demonstrate that a modest
	amount of real-world training can substantially improve performance during
	deployment, and this enables a real A1 quadrupedal robot to autonomously fine-tune
	multiple locomotion skills in a range of environments, including an outdoor lawn
	and a variety of indoor terrains.
</td>

<td>
	<h3> Paper: [<a href="2021_Finetuning_Locomotion.pdf">PDF</a>] &nbsp; &nbsp; &nbsp; Code: [<a href="https://github.com/lauramsmith/fine-tuning-locomotion">GitHub</a>] &nbsp; &nbsp; &nbsp; Preprint: [<a href="https://arxiv.org/abs/2110.05457">arXiv</a>] </h3>
</td>

<tr>
		<h3 style="margin-bottom:10px;">Videos</h3>
		<iframe width="560" height="315" src="https://www.youtube.com/embed/1EUQD7nYfLM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</tr>
	
<br>
<br>

<h3 style="margin-bottom:0px;">Bibtex</h3>
<pre>
@misc{
    smith2021legged,
    title={Legged Robots that Keep on Learning: Fine-Tuning Locomotion Policies in the Real World}, 
    author={Laura Smith and J. Chase Kew and Xue Bin Peng and Sehoon Ha and Jie Tan and Sergey Levine},
    year={2021},
    eprint={2110.05457},
    archivePrefix={arXiv},
    primaryClass={cs.RO}
}
</pre>
